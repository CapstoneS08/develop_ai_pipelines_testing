{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb91052",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, confusion_matrix\n",
    "\n",
    "from pipeline import train_seq2seq_model, predict_scores_for_spans\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "train_jsonl = os.path.join(project_root, \"data\", \"processed\", \"training\", \"whatsapp_aspect_training_300.jsonl\")\n",
    "val_jsonl   = os.path.join(project_root, \"data\", \"processed\", \"validation\", \"validation_gpt_50.jsonl\")\n",
    "\n",
    "# Span-level inputs from preprocessing\n",
    "preproc_results_dir = os.path.join(project_root, \"notebooks\", \"cs_scoring\", \"llm_preprocessing\", \"results\")\n",
    "flat_spans_path = os.path.join(preproc_results_dir, \"b2b_feedback_attribute_spans_flat.csv\")\n",
    "\n",
    "# Where to store seq2seq outputs\n",
    "results_dir = os.path.join(os.getcwd(), \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(train_jsonl, os.path.exists(train_jsonl))\n",
    "print(val_jsonl, os.path.exists(val_jsonl))\n",
    "print(flat_spans_path, os.path.exists(flat_spans_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ad149",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_df = pd.read_csv(flat_spans_path)\n",
    "spans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = [\n",
    "    \"google/flan-t5-base\",\n",
    "    \"facebook/bart-base\",\n",
    "]\n",
    "models_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_dirs = {}\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    safe_name = model_name.replace(\"/\", \"_\")\n",
    "    output_dir = os.path.join(results_dir, safe_name)\n",
    "\n",
    "    print(f\"üöÄ Training {model_name}\")\n",
    "    trainer, tokenizer, model = train_seq2seq_model(\n",
    "        model_name=model_name,\n",
    "        train_jsonl=train_jsonl,\n",
    "        val_jsonl=val_jsonl,\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_batch_size=8,\n",
    "        learning_rate=5e-5,\n",
    "    )\n",
    "\n",
    "    trained_model_dirs[model_name] = output_dir\n",
    "\n",
    "trained_model_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_predictions = {}\n",
    "\n",
    "for model_name, model_dir in trained_model_dirs.items():\n",
    "    print(f\"\\nüîÆ Predicting with {model_name}\")\n",
    "    pred_df = predict_scores_for_spans(model_path=model_dir, spans_df=spans_df)\n",
    "\n",
    "    spans_predictions[model_name] = pred_df\n",
    "\n",
    "    out_csv = os.path.join(results_dir, f\"{model_name.replace('/', '_')}_pred.csv\")\n",
    "    pred_df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"Saved to:\", out_csv)\n",
    "\n",
    "spans_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ac723",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_records = []\n",
    "with open(val_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        val_records.append(json.loads(line))\n",
    "\n",
    "val_df = pd.DataFrame(val_records)[[\"attribute\", \"text_span\", \"score\"]]\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== FINAL MODEL METRICS =====\\n\")\n",
    "\n",
    "for model_name, pred_df in spans_predictions.items():\n",
    "    merged = val_df.merge(\n",
    "        pred_df[[\"attribute\", \"text_span\", \"pred_score\"]],\n",
    "        on=[\"attribute\", \"text_span\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        print(f\"‚ö†Ô∏è No matching spans for model {model_name}\")\n",
    "        continue\n",
    "\n",
    "    y_true = merged[\"score\"].astype(int).values\n",
    "    y_pred = merged[\"pred_score\"].astype(int).values\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,2,3,4,5])\n",
    "\n",
    "    print(f\"\\nüéØ MODEL: {model_name}\")\n",
    "    print(f\"Total matches: {len(merged)}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74c373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f8ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
