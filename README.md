# AI Pipelines Testing
Testing for AI models in the Develop phase of the double diamond, consisting of three major NLP pipelines:

1. **CS Scoring Model**  
   - Mapping stage (Sentiment vs. LLM)  
   - Issue-resolution smoothing  
   - Linear regression to final CS Performance Score (1â€“5)

2. **Summarization Model**  
   - Extracts *only* actionable improvement comments from long WhatsApp/feedback text

3. **Issue Tracking Model**  
   - Classifies incoming text into one of six issue types  
   - Auto-fills structured issue fields for internal tracking

The system is modular, configuration-driven, and designed for reproducible experiments.

---

## ğŸ“ Repository Structure

    ecoplus-cs-pipeline/
    â”œâ”€ README.md
    â”œâ”€ .gitignore
    â”œâ”€ requirements.txt
    â”œâ”€ .env.example
    â”‚
    â”œâ”€ configs/
    â”‚  â”œâ”€ cs_scoring/
    â”‚  â”‚  â”œâ”€ sentiment_baseline.yaml
    â”‚  â”‚  â”œâ”€ llm_zero_shot.yaml
    â”‚  â”‚  â””â”€ llm_finetuned.yaml
    â”‚  â”œâ”€ summarizer.yaml
    â”‚  â””â”€ issue_tracking.yaml
    â”‚
    â”œâ”€ data/
    â”‚  â”œâ”€ raw/
    â”‚  â””â”€ processed/
    â”‚
    â”œâ”€ src/
    â”‚  â”œâ”€ common/
    â”‚  â”‚  â”œâ”€ io_utils.py
    â”‚  â”‚  â”œâ”€ preprocessing.py
    â”‚  â”‚  â”œâ”€ metrics.py
    â”‚  â”‚  â”œâ”€ gpt_utils.py
    â”‚  â”‚  â””â”€ model_utils.py
    â”‚  â”‚
    â”‚  â”œâ”€ cs_scoring/
    â”‚  â”‚  â”œâ”€ full_pipeline.py
    â”‚  â”‚  â”œâ”€ mapping/
    â”‚  â”‚  â”‚  â”œâ”€ sentiment_pipeline.py
    â”‚  â”‚  â”‚  â”œâ”€ llm_zero_shot.py
    â”‚  â”‚  â”‚  â”œâ”€ llm_finetuned.py
    â”‚  â”‚  â”‚  â””â”€ evaluate_mapping.py
    â”‚  â”‚  â”œâ”€ scoring/
    â”‚  â”‚  â”‚  â”œâ”€ issue_resolution.py
    â”‚  â”‚  â”‚  â”œâ”€ linreg_model.py
    â”‚  â”‚  â”‚  â””â”€ evaluate_cs.py
    â”‚  â”‚  â””â”€ experiments_legacy/
    â”‚  â”‚      â””â”€ README.md
    â”‚  â”‚
    â”‚  â”œâ”€ summarization/
    â”‚  â”‚  â”œâ”€ summarizer_model.py
    â”‚  â”‚  â””â”€ evaluate_summ.py
    â”‚  â”‚
    â”‚  â””â”€ issue_tracking/
    â”‚     â”œâ”€ issue_model.py
    â”‚     â””â”€ evaluate_issues.py
    â”‚
    â”œâ”€ reports/
    â”‚  â””â”€ figures/
    â”‚
    â””â”€ notebooks/
       â”œâ”€ 01_eda.ipynb
       â”œâ”€ 02_cs_scoring.ipynb
       â”œâ”€ 03_summarization.ipynb
       â””â”€ 04_issue_tracking.ipynb

---

## âš™ï¸ Setup Instructions

1. **Install dependencies**

    pip install -r requirements.txt

2. **Create your `.env`**

    cp .env.example .env

Fill in at least:

    OPENAI_API_KEY=your-key-here

---

## ğŸ§  1. CS Scoring Model â€“ Overview

The **CS Performance Score Model** predicts a 1â€“5 score for each customer/project based on incoming WhatsApp and feedback messages.

It consists of three stages:

1. **Stage 1 â€“ Mapping**: Text â†’ aspect scores (1â€“5) for `Product`, `Service`, `Delivery`, `Payment`, â€¦  
2. **Stage 2 â€“ Issue Resolution Smoothing**: Apply recurrence to combine historical and new signals.  
3. **Stage 3 â€“ Linear Regression**: Map features to final CS Performance Score (1â€“5).

---

### Stage 1 â€” Mapping (Text â†’ Aspect Scores 1â€“5)

We extract scores for predefined aspects:

- Product  
- Service  
- Delivery  
- Payment  

There are two main mapping families: **sentiment-based** and **LLM-based**.

---

### A. Sentiment-based Mapping (current pipeline)

Flow:

1. LLM preprocessing extracts points and assigns aspects.  
2. A sentiment model (Cardiff or other HF models) produces sentiment scores for each point.  
3. A follow-up LLM call maps sentiment scores back to aspect scores from 1â€“5.

This tests whether sentiment signal can reliably approximate aspect quality.

Notes:

- Baseline model: `cardiffnlp/twitter-roberta-base-sentiment-latest`  
- Other HF models can be plugged in via config.  
- Sentiment model fine-tuning is planned but not yet attempted.

---

### B. LLM-based Mapping (current pipeline)

Flow:

1. The LLM directly classifies text into aspect scores 1â€“5.  
2. Output format (example):

       {
         "Service": 1,
         "Payment": 5
       }

Variants:

- **Zero-shot** LLM mapping (prompt-only).  
- **Finetuned classifier** (dataset prepared; to be integrated).

This is the **primary mapping approach** going forward.

---

### Stage 2 â€” Issue Resolution Smoothing

Implements the recurrence:

    x_{t+1} = Î± x_t + k (1 âˆ’ Î±) Î©_{t+1}

where:

- `x_t` = previous score  
- `Î©_{t+1}` = new observation  
- `Î±` = smoothing factor  
- `k` = modifier depending on satisfaction (not satisfied / neutral / satisfied)

This models how successfully resolved issues influence ongoing satisfaction.

---

### Stage 3 â€” Linear Regression Model

Uses:

- Aspect scores from Stage 1  
- Issue-resolution features from Stage 2  
- (Optionally) additional structured features

Output:

- Final **CS Performance Score** (1â€“5) per customer / project.  
- Can also generate a time-series of scores for trend visualisation.

---

## ğŸ“Š Metrics

### Mapping metrics (Stage 1)

- Per-aspect **MAE** (mean absolute error)  
- Per-aspect **accuracy within Â±1 point**  
- Macro-averaged MAE across aspects  
- Optional: **Quadratic Weighted Kappa (QWK)** per aspect

### CS Performance metrics (Stage 3)

- **MAE**  
- **RMSE**  
- **RÂ²**  
- **Accuracy within Â±1 point**  
- **QWK**

These metrics are computed in `src/cs_scoring/mapping/evaluate_mapping.py` and `src/cs_scoring/scoring/evaluate_cs.py`.

---

## ğŸ§ª Experiments Legacy (Mapping Stage)

These early experiments attempted to compute CX/CS scores **without aspect mapping** and were later abandoned. They are kept under `src/cs_scoring/experiments_legacy/` for reference.

Common issue across all:  

> Sentiment alone does not provide aspect-level information, so downstream CS scores were unstable and hard to interpret.

### Legacy Experiment 1 â€” Combined Chunk â†’ Sentiment â†’ LLM â†’ CX Score

- Combined positive/negative content into a single chunk.  
- Produced one sentiment score for the chunk.  
- Asked LLM to map this sentiment to a CX score directly (1â€“5), without aspects.

**Reason abandoned:** Could not distinguish between Delivery vs Service vs Product problems; good vs bad experiences were flattened.

---

### Legacy Experiment 2 â€” P/N/N Lists â†’ Sentiment â†’ LLM â†’ CX Score

- LLM produced:

  - List of positive points  
  - List of negative points  
  - (Sometimes) list of neutral points  

- Sentiment model scored each list / point.  
- LLM mapped aggregated sentiment back to a single CX score.

**Reason abandoned:**  
Although more granular than Experiment 1, there were still **no aspect labels**; CS scores were difficult to relate to specific operational issues.

---

### Legacy Experiment 3 â€” Combined Chunk â†’ Sentiment â†’ Math â†’ CX Score

- Produced one sentiment score for the entire chunk.  
- Used manual mathematical mappings to convert sentiment probability into a 1â€“5 CX score.  

  Example idea:

      CX = 5 * P(positive) + 3 * P(neutral) + 1 * P(negative)

**Reason abandoned:**  
Heuristic mapping was brittle and behaved badly for mixed or ambiguous comments. Without aspect separation, there was no way to know *why* a score changed.

---

## ğŸ§ª Legacy vs Current Pipelines â€” Quick Summary

**CS Scoring Model**

- **Mapping**
  - **Sentiment-analysis**
    - Current pipeline  
      - LLM aspect extraction â†’ sentiment model â†’ LLM mapping â†’ aspect scores.  
    - Experiments-legacy  
      - Chunk sentiment â†’ LLM â†’ CX (no aspects)  
      - P/N/N â†’ LLM â†’ CX (no aspects)  
      - Sentiment â†’ math â†’ CX (no aspects)
  - **LLM**
    - Current pipeline  
      - Zero-shot LLM, and a finetuned classifier (to be integrated).  
    - Experiments-legacy  
      - None (LLM mapping was introduced after aspect design).

- **Linear Regression Model**
  - Uses aspect and issue-resolution features to predict final CS score.

- **Overall Pipeline**
  - Mapping â†’ Issue Resolution Smoothing â†’ Linear Regression â†’ Final CS Performance Score.

---

## ğŸ“ 2. Summarization Model

Goal: extract **only improvement comments** from long messages.

Example:

- Input:  
  â€œDelivery was slow but product is good. Please update us faster next time.â€  

- Output:  
  â€œImprove delivery speed and response time.â€

Implementation (high level):

- Uses an LLM with instruction-style prompts to generate concise, actionable improvement comments.  
- May optionally use aspect mapping to check coverage of key issues.

Evaluation:

- **Expert ratings** on a small sample:  
  - Actionability (1â€“5)  
  - Faithfulness (1â€“5)  
  - Conciseness (1â€“5)  
- **Aspect coverage**:  
  - Compare aspects in original text vs summary (precision / recall).

---

## ğŸ› ï¸ 3. Issue Tracking Model

Classifies incoming text into the six internal **issue types** (from the issue tracker sheet), e.g.:

- Delay  
- Product Quality  
- Stock Issues  
- Service Issues  
- Fulfillment Error  
- Payments

Then auto-fills structured fields in the issue tracker, such as:

- Created By  
- Created At  
- To Inform  
- Assigned To  
- Resolved At  
- Status to Close  
- Closed At  
- Remarks  

Evaluation:

- **Macro-F1** for issue-type classification.  
- **Per-field accuracy** for each structured field.  
- **Record-level exact match**: percentage of rows where all fields are correct.

---

## ğŸš€ Running the Pipelines (high level)

Exact commands will depend on how you implement the scripts, but the general pattern is:

- **CS Scoring: Mapping evaluation**

      python src/cs_scoring/mapping/evaluate_mapping.py

- **CS Scoring: Full pipeline**

      python src/cs_scoring/full_pipeline.py
      python src/cs_scoring/scoring/evaluate_cs.py

- **Summarizer**

      python src/summarization/summarizer_model.py
      python src/summarization/evaluate_summ.py

- **Issue Tracking**

      python src/issue_tracking/issue_model.py
      python src/issue_tracking/evaluate_issues.py

You can also use the notebooks in `notebooks/` for exploratory runs and sanity checks.

---

## ğŸ”­ Future Work

- Integrate and compare finetuned LLM mapping vs zero-shot.  
- Fine-tune sentiment models for better domain adaptation.  
- Incorporate issue-tracking features directly into CS scoring.  
- Build a labelled dataset for supervised summarization and potential instruction fine-tuning.  
- Add more aspects and support per-customer trend dashboards.

---

## ğŸ“„ License / Usage

This repository is intended for the Ecoplus Capstone project and internal academic use.  
External use or distribution should be approved by the project stakeholders.
